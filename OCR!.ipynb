{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN8JZeNPYg+UUzmviMf7tk7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f4ATOl7CICKB","executionInfo":{"status":"ok","timestamp":1766158499846,"user_tz":-330,"elapsed":2501,"user":{"displayName":"Harshath.A","userId":"14198956414191235415"}},"outputId":"dc1930f6-5fe1-488f-e9db-cfd8f2496242"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["import torch\n","torch.cuda.is_available()\n"]},{"cell_type":"code","source":["!pip install -U transformers datasets torch rapidfuzz opencv-python jiwer pillow torchvision torchaudio --quiet\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ylxlAFmsJMHt","executionInfo":{"status":"ok","timestamp":1766158518785,"user_tz":-330,"elapsed":18937,"user":{"displayName":"Harshath.A","userId":"14198956414191235415"}},"outputId":"145bb2d0-58ba-4a5a-d9cb-899c38845ca9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gradio 5.50.0 requires pillow<12.0,>=8.0, but you have pillow 12.0.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["!pip uninstall -y pillow\n","!pip install \"pillow==11.3.0\" --no-cache-dir --quiet\n","\n","\n","from PIL import Image\n","print(Image.__version__)\n","\n","\n","!pip install transformers datasets torch rapidfuzz opencv-python jiwer pillow==11.3.0 --quiet\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e2tH6tZZPZ6p","executionInfo":{"status":"ok","timestamp":1766159341549,"user_tz":-330,"elapsed":27889,"user":{"displayName":"Harshath.A","userId":"14198956414191235415"}},"outputId":"e2d2605b-de72-484b-eede-f348c1a3e792"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: pillow 11.3.0\n","Uninstalling pillow-11.3.0:\n","  Successfully uninstalled pillow-11.3.0\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m119.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h11.3.0\n"]}]},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","\n","# =======================================\n","# Medicine OCR + Correction + Information Pipeline\n","# CLEAN & CORRECTED NOTEBOOK (Same filenames preserved)\n","\n","\n","# =======================================\n","# STEP 1: Imports\n","# =======================================\n","import random\n","import torch\n","import pandas as pd\n","import cv2\n","import matplotlib.pyplot as plt\n","\n","from PIL import Image\n","from datasets import Dataset\n","from rapidfuzz import process, fuzz\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSeq2SeqLM,\n","    DataCollatorForSeq2Seq,\n","    Trainer,\n","    TrainingArguments,\n","    pipeline,\n","    TrOCRProcessor,\n","    VisionEncoderDecoderModel\n",")\n","\n","# Device\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# =======================================\n","# STEP 2: Load medicine dataset (names)\n","# =======================================\n","df = pd.read_csv(\"/content/A_Z_medicines_dataset_of_India.csv\")\n","\n","# =======================================\n","# STEP 3: OCR-style noise generator (cleaned)\n","# =======================================\n","def add_noise(text, noise_factor=0.4):\n","    substitutions = {\n","        \"0\": \"O\", \"O\": \"0\",\n","        \"1\": \"I\", \"I\": \"1\",\n","        \"5\": \"S\", \"S\": \"5\",\n","        \"8\": \"B\", \"B\": \"8\",\n","        \"c\": \"e\", \"e\": \"c\",\n","        \"l\": \"I\"\n","    }\n","\n","    text = list(text)\n","    num_noises = max(1, int(len(text) * noise_factor))\n","\n","    for _ in range(num_noises):\n","        op = random.choice([\"swap\", \"delete\", \"insert\", \"replace\", \"duplicate\", \"caseflip\", \"space\"])\n","        i = random.randint(0, len(text) - 1)\n","\n","        if op == \"swap\" and i < len(text) - 1:\n","            text[i], text[i + 1] = text[i + 1], text[i]\n","        elif op == \"delete\":\n","            text[i] = \"\"\n","        elif op == \"insert\":\n","            text.insert(i, random.choice(\"abcdefghijklmnopqrstuvwxyz\"))\n","        elif op == \"replace\":\n","            text[i] = substitutions.get(text[i], random.choice(\"abcdefghijklmnopqrstuvwxyz\"))\n","        elif op == \"duplicate\":\n","            text.insert(i, text[i])\n","        elif op == \"caseflip\":\n","            text[i] = text[i].upper() if text[i].islower() else text[i].lower()\n","        elif op == \"space\":\n","            text.insert(i, \" \")\n","\n","    return \"\".join(text).strip()\n","\n","# =======================================\n","# STEP 4: Create synthetic dataset\n","# =======================================\n","clean_names = df['name'].dropna().unique().tolist()[:9000]\n","\n","data = [\n","    {\"input_text\": add_noise(name.lower()), \"target_text\": name.lower()}\n","    for name in clean_names\n","]\n","\n","dataset = Dataset.from_list(data)\n","print(\"âœ… Synthetic dataset created:\", len(dataset))\n","\n","# =======================================\n","# STEP 5: Load ByT5 model\n","# =======================================\n","BASE_MODEL = \"google/byt5-small\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n","model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL).to(DEVICE)\n","\n","# =======================================\n","# STEP 6: Tokenization (padding fixed)\n","# =======================================\n","def preprocess_function(examples):\n","    model_inputs = tokenizer(\n","        examples[\"input_text\"],\n","        max_length=64,\n","        padding=\"max_length\",\n","        truncation=True\n","    )\n","\n","    labels = tokenizer(\n","        examples[\"target_text\"],\n","        max_length=64,\n","        padding=\"max_length\",\n","        truncation=True\n","    ).input_ids\n","\n","    labels = [\n","        [(l if l != tokenizer.pad_token_id else -100) for l in label]\n","        for label in labels\n","    ]\n","\n","    model_inputs[\"labels\"] = labels\n","    return model_inputs\n","\n","\n","tokenized_dataset = dataset.map(\n","    preprocess_function,\n","    batched=True,\n","    remove_columns=[\"input_text\", \"target_text\"]\n",")\n","\n","# =======================================\n","# STEP 7: Training\n","# =======================================\n","training_args = TrainingArguments(\n","    output_dir=\"/content/byt5-medicine-corrector\",\n","    eval_strategy=\"no\",\n","    learning_rate=1e-4,\n","    per_device_train_batch_size=8,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n","    logging_steps=50,\n","    save_strategy=\"epoch\",\n","    optim=\"adafactor\",\n","    report_to=\"none\"\n",")\n","\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator\n",")\n","\n","trainer.train()\n","trainer.save_model(\"/content/byt5-medicine-corrector\")\n","print(\"âœ… Model saved at /content/byt5-medicine-corrector\")\n","\n","# =======================================\n","# STEP 8: Load fine-tuned corrector (SAFE)\n","# =======================================\n","MODEL_PATH = \"/content/byt5-medicine-corrector\"\n","\n","corrector_tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n","corrector_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH, local_files_only=True).to(DEVICE)\n","corrector_model.eval()\n","\n","\n","def correct_text(text):\n","    inputs = corrector_tokenizer(text.lower(), return_tensors=\"pt\").to(DEVICE)\n","    with torch.no_grad():\n","        outputs = corrector_model.generate(**inputs, max_length=64)\n","    return corrector_tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","# =======================================\n","# STEP 9: Load Medicine details dataset\n","# =======================================\n","df1 = pd.read_csv(\"/content/Medicine_Details.csv\")\n","df1.columns = [c.strip().replace(\" \", \"_\") for c in df1.columns]\n","\n","# =======================================\n","# STEP 10: Fuzzy match + explanation\n","# =======================================\n","\n","def get_closest_medicine(query, df, limit=3):\n","    names = df[\"Medicine_Name\"].tolist()\n","    return process.extract(query, names, scorer=fuzz.WRatio, limit=limit)\n","\n","\n","def generate_drug_explanation(drug_name, uses, side_effects, composition=None):\n","    return (\n","        f\"The medicine '{drug_name}' is composed of {composition or 'active ingredients'}. \"\n","        f\"It is used for {uses}. Common side effects include {side_effects}. \"\n","        \"This information is for educational purposes only.\"\n","    )\n","\n","\n","generator = pipeline(\n","    \"text2text-generation\",\n","    model=\"google/flan-t5-base\",\n","    max_new_tokens=80,\n","    device=0 if DEVICE == \"cuda\" else -1\n",")\n","\n","\n","def make_readable_explanation(text):\n","    return generator(f\"Rephrase simply and professionally: {text}\")[0]['generated_text']\n","\n","\n","def get_drug_info(ocr_text):\n","    ocr_text = ocr_text.lower().strip()\n","    matches = get_closest_medicine(ocr_text, df1)\n","    best_match, score, idx = matches[0]\n","    record = df1.iloc[idx]\n","\n","    explanation = generate_drug_explanation(\n","        record[\"Medicine_Name\"],\n","        record[\"Uses\"],\n","        record[\"Side_effects\"],\n","        record.get(\"Composition\")\n","    )\n","\n","    return {\n","        \"OCR Input\": ocr_text,\n","        \"Matched Medicine\": record[\"Medicine_Name\"],\n","        \"Match Confidence\": score,\n","        \"Generated Explanation\": make_readable_explanation(explanation)\n","    }\n","\n","# =======================================\n","# STEP 11: OCR using TrOCR (PRINTED)\n","# =======================================\n","from google.colab import files\n","uploaded = files.upload()\n","img_path = list(uploaded.keys())[0]\n","\n","img = cv2.imread(img_path)\n","gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","clahe = cv2.createCLAHE(3.0, (8, 8))\n","gray = clahe.apply(gray)\n","blur = cv2.GaussianBlur(gray, (5, 5), 0)\n","thresh = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 31, 2)\n","\n","processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-large-printed\")\n","trocr_model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-large-printed\").to(DEVICE)\n","\n","trocr_model.eval()\n","img_pil = Image.fromarray(thresh).convert(\"RGB\")\n","\n","pixel_values = processor(images=img_pil, return_tensors=\"pt\").pixel_values.to(DEVICE)\n","\n","with torch.no_grad():\n","    ids = trocr_model.generate(pixel_values)\n","\n","ocr_output = processor.batch_decode(ids, skip_special_tokens=True)[0]\n","\n","# =======================================\n","# STEP 12: Final pipeline\n","# =======================================\n","corrected = correct_text(ocr_output)\n","result = get_drug_info(corrected)\n","\n","print(\"ğŸ§  Raw OCR:\", ocr_output)\n","print(\"âœ¨ Corrected:\", corrected)\n","print(\"âœ… Medicine:\", result[\"Matched Medicine\"], f\"({result['Match Confidence']:.1f}%)\")\n","print(\"ğŸ’¬ Explanation:\", result[\"Generated Explanation\"])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":400},"id":"UaYSFw3hTS1g","outputId":"503fcafc-a9b6-4138-c92d-10713e20b0c4","executionInfo":{"status":"error","timestamp":1766246891760,"user_tz":-330,"elapsed":32441,"user":{"displayName":"Harshath.A","userId":"14198956414191235415"}}},"execution_count":1,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'rapidfuzz'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1798211630.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrapidfuzz\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuzz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m from transformers import (\n\u001b[1;32m     22\u001b[0m     \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rapidfuzz'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["!pip install \\\n","  transformers \\\n","  datasets \\\n","  torch \\\n","  rapidfuzz \\\n","  opencv-python \\\n","  jiwer \\\n","  pillow==11.3.0 \\\n","  torchvision \\\n","  torchaudio \\\n","  --quiet\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ot8wR7s2AeoA","executionInfo":{"status":"ok","timestamp":1766246911577,"user_tz":-330,"elapsed":15387,"user":{"displayName":"Harshath.A","userId":"14198956414191235415"}},"outputId":"ecb2d367-2900-4a06-c57c-36812dfa7b97"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/3.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m131.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]}]}